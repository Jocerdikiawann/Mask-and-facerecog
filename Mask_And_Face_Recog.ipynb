{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask_And_Face_Recog.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mAI6DEYFRZAO",
        "ydHnWlmZP7k1",
        "y-xaDmUXQbvw",
        "kUEm9kJOQqh-",
        "PF5jETZLQmem"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Pre Proses**"
      ],
      "metadata": {
        "id": "L2B5_2fgPLvm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LefHWAXrYXUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e2c9964-2b06-4b96-ebe3-c4f74733b941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mask-Face-recog-Project'...\n",
            "remote: Enumerating objects: 2602, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 2602 (delta 2), reused 0 (delta 0), pack-reused 2596\u001b[K\n",
            "Receiving objects: 100% (2602/2602), 165.32 MiB | 31.18 MiB/s, done.\n",
            "Resolving deltas: 100% (569/569), done.\n",
            "Checking out files: 100% (2765/2765), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/debajyoty/Mask-Face-recog-Project.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ./Mask-Face-recog-Project/"
      ],
      "metadata": {
        "id": "R87EQhSdZOYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37751a1-a67d-465c-a8b9-9a3bd91b1bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask-Face-recog-Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install face-recognition"
      ],
      "metadata": {
        "id": "8ThehGP_Zdxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888e6c4b-5123-4f1c-d246-f9af00e12160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face-recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face-recognition) (7.1.2)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 100.1 MB 35 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face-recognition) (7.1.2)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face-recognition) (19.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face-recognition) (1.21.5)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566186 sha256=c71ccf1a3ad3d6e373e1567c9542ec8f9428f320da2f4b9b68b9868d886dbe98\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/81/3c/884bcd5e1c120ff548d57c2ecc9ebf3281c9a6f7c0e7e7947a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "tQY5yBSuaCDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd Recog_Train/"
      ],
      "metadata": {
        "id": "lUcgOnV4ehx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.remove('.DS_Store')"
      ],
      "metadata": {
        "id": "HG9r89sRdZ5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "id": "4HK8-olhfUvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09dd9cee-6553-48d9-bd5a-2b9a8c37e315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask-Face-recog-Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Library**"
      ],
      "metadata": {
        "id": "WtkmMoAgPRhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Base Library\n",
        "import os\n",
        "import cv2\n",
        "import face_recognition\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "import imutils\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "from threading import Timer\n",
        "import shutil\n",
        "import docopt\n",
        "from sklearn import svm\n",
        "#Video library\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode"
      ],
      "metadata": {
        "id": "oWVnyYPjT6OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State Image"
      ],
      "metadata": {
        "id": "GEupdYgJPbLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = os.getcwd() + \"/Recog_Train\"\n",
        "\n",
        "images = []\n",
        "class_names = []\n",
        "encode_list = []\n",
        "encode_list_cl = []\n",
        "myList = os.listdir(img_path)\n",
        "\n",
        "#print(myList)\n",
        "\n",
        "for subdir in myList:\n",
        "  path = img_path + '/' + subdir + '/'\n",
        "  for img in os.listdir(path):\n",
        "    img_pic = path + img\n",
        "    class_names.append(subdir)\n",
        "    print(\"class names : \",img_pic)\n",
        "    cur_img = cv2.imread(img_pic)\n",
        "    cur_img = cv2.cvtColor(cur_img , cv2.COLOR_BGR2RGB)\n",
        "    images.append(cur_img)"
      ],
      "metadata": {
        "id": "XYxQwpf3Uv-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5cb1cfc-3b25-45a9-d7e5-500a49b75d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Jokowi/jokowipakemasker4.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Jokowi/jokowi.jpeg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Jokowi/jokowi.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Jokowi/jokowipakemasker.jpeg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Jokowi/jokowi.png\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Jokowi/jokowipakemasker.png\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Jokowi/jokowipakemasker.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Jokowi/jokowi3.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Jokowi/jokowi2.jpeg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Raffi/raffi.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Raffi/raffi6.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Raffi/raffi4.jpeg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Raffi/raffi2.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Raffi/raffi5.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Raffi/raffipakemasker3.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Raffi/raffipakemasker.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Raffi/raffipakemasker2.png\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Raffi/raffi3.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Kim Jong Un/Kim.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Kim Jong Un/Kim 2.jpg\n",
            "class names :  /content/Mask-Face-recog-Project/Recog_Train/Kim Jong Un/Kim 3.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove .ipnb_checkpoint\n",
        "Sering terjadi error ketika mendelete image yaitu tidak terbacanya image, dikarenakan ada folder .ipnb_checkpoint yang mengganggu proses pengambilan/pengumpulan image ke state.\n",
        "Running ketika terjadi error tersebut"
      ],
      "metadata": {
        "id": "mAI6DEYFRZAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmdir \"/content/Mask-Face-recog-Project/Recog_Train/Shakira/.ipynb_checkpoints\""
      ],
      "metadata": {
        "id": "Drd1TQI4DsVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mask Detection function"
      ],
      "metadata": {
        "id": "GWqN9tCIPn0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.imshow()\n",
        "\n",
        "#for img in images:\n",
        "    \n",
        "\n",
        "def detect_and_predict_mask(frame, faceNet, maskNet,threshold):\n",
        "\t# grab the dimensions of the frame and then construct a blob\n",
        "\t# from it\n",
        "\tglobal detections \n",
        "\t(h, w) = frame.shape[:2]\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),(104.0, 177.0, 123.0))\n",
        "\t# pass the blob through the network and obtain the face detections\n",
        "\tfaceNet.setInput(blob)\n",
        "\tdetections = faceNet.forward()\n",
        "\n",
        "\t# initialize our list of faces, their corresponding locations,\n",
        "\t# and the list of predictions from our face mask network\n",
        "\tfaces = []\n",
        "\tlocs = []\n",
        "\tpreds = []\n",
        "\t# loop over the detections\n",
        "\tfor i in range(0, detections.shape[2]):\n",
        "\t\t# extract the confidence (i.e., probability) associated with\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t# filter out weak detections by ensuring the confidence is\n",
        "\t\t# greater than the minimum confidence\n",
        "\t\tif confidence >threshold:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the object\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t# ensure the bounding boxes fall within the dimensions of\n",
        "\t\t\t# the frame\n",
        "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
        "\t\t\tface = frame[startY:endY, startX:endX]\n",
        "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\t\tface = img_to_array(face)\n",
        "\t\t\tface = preprocess_input(face)\n",
        "\t\t\tface = np.expand_dims(face, axis=0)\n",
        "            \n",
        "\t\t\t# add the face and bounding boxes to their respective\n",
        "\t\t\t# lists\n",
        "\t\t\tlocs.append((startX, startY, endX, endY))\n",
        "\t\t\t# print(\"tes\",maskNet.predict(face)[0].tolist())\n",
        "\t\t\tpreds.append(maskNet.predict(face)[0].tolist())\n",
        "\treturn (locs, preds)\n",
        "# SETTINGS\n",
        "MASK_MODEL_PATH=\"./masksdetection-master/model/mask_model.h5\"\n",
        "FACE_MODEL_PATH=\"./masksdetection-master/face_detector\"\n",
        "SOUND_PATH=\"./masksdetection-master/sounds/alarm.wav\" \n",
        "THRESHOLD = 0.5\n",
        "\n",
        "# Load Sounds\n",
        "#mixer.init()\n",
        "#sound = mixer.Sound(SOUND_PATH)\n",
        "from os.path import dirname, join\n",
        "\n",
        "protoPath = \"./deploy.prototxt\"\n",
        "weightsPath = \"./res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "#prototxtPath = \"C:\\masksdetection-master\\masksdetection-master\\face_detector\\deploy.prototxt.txt\"\n",
        "#weightsPath = os.path.sep.join([FACE_MODEL_PATH,\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "faceNet = cv2.dnn.readNet(protoPath, weightsPath)\n",
        "\n",
        "# load the face mask detector model from disk\n",
        "print(\"[INFO] loading face mask detector model...\")\n",
        "maskNet = load_model(MASK_MODEL_PATH)\n",
        "\n",
        "# initialize the video stream and allow the camera sensor to warm up\n",
        "print(\"[INFO] starting video stream...\")\n",
        "# vs = VideoStream(0).start()\n",
        "# time.sleep(2.0)\n",
        "        \n",
        "#for cl in myList :\n",
        " #   cur_img = cv2.imread(f'{img_path} / {cl}' )\n",
        " #   images.append([cur_img])\n",
        "        \n",
        "def find_encodings(images) :\n",
        "\tfor img in images:\n",
        "\t\t# heightFace,widthFace,_ = img.shape\n",
        "\t\t# face_location = (0,heightFace,widthFace,0)\n",
        "\t\tencodings = face_recognition.face_encodings(img)[0]\n",
        "\t\t# if(len(encodings)>0):\n",
        "\t\t# \tencode_list.append(encodings[0])\n",
        "\t\t# else:\n",
        "\t\t# \tprint(\"No faces found in the image!\")\n",
        "\t\treturn encode_list\n",
        "    \n",
        "encodeListKnown = find_encodings(images)\n",
        " \n",
        "# cap = cv2.VideoCapture(0)\n",
        "\n",
        "# while True : \n",
        "\n",
        "\t\t\n",
        "\t\t# key = cv2.waitKey(1) & 0xFF\n",
        "\t\t\n",
        "\t\t# if key == ord(\"q\"):\n",
        "\t\t# \t\tbreak\n",
        "# cv2.destroyAllWindows()\n",
        "# vs.stop()"
      ],
      "metadata": {
        "id": "WFx6xTCmfxdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c41481-6433-4497-92eb-343365183539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading face detector model...\n",
            "[INFO] loading face mask detector model...\n",
            "[INFO] starting video stream...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Helper Stream**"
      ],
      "metadata": {
        "id": "J2tspForO-Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "nU81K19COcA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Webcam Image Helper"
      ],
      "metadata": {
        "id": "ydHnWlmZP7k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data) \n",
        "  \n",
        "  # call our darknet helper on webcam image\n",
        "  detections, width_ratio, height_ratio = darknet_helper(img, width, height)\n",
        "\n",
        "  # loop through detections and draw them on webcam image\n",
        "  for label, confidence, bbox in detections:\n",
        "    left, top, right, bottom = bbox2points(bbox)\n",
        "    left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)\n",
        "    cv2.rectangle(img, (left, top), (right, bottom), class_colors[label], 2)\n",
        "    cv2.putText(img, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
        "                      (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                      class_colors[label], 2)\n",
        "  # save image\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ],
      "metadata": {
        "id": "oSwz7nxKPxrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Stream Helper"
      ],
      "metadata": {
        "id": "aSJeK8lgQGNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "Asiu8yfAQGsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection Mask and face Recog"
      ],
      "metadata": {
        "id": "6cUgqZHAPtOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Image Detection (Beta)\n",
        "belum selesai jangan di running"
      ],
      "metadata": {
        "id": "y-xaDmUXQbvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = []\n",
        "names = []\n",
        "for person in myList:\n",
        "  pix = os.listdir(img_path+\"/\"+person)\n",
        "  for person_img in pix:\n",
        "    # Get the face encodings for the face in each image file\n",
        "    path = img_path+\"/\"+person+\"/\"+person_img\n",
        "    face = face_recognition.load_image_file(path)\n",
        "    face_bounding_boxes = face_recognition.face_locations(face)\n",
        "\n",
        "    # If training image contains exactly one face\n",
        "    if len(face_bounding_boxes) == 1:\n",
        "        face_enc = face_recognition.face_encodings(face)[0]\n",
        "        # Add face encoding for current image \n",
        "        # with corresponding label (name) to the training data\n",
        "        encodings.append(face_enc)\n",
        "        names.append(person)\n",
        "    else:\n",
        "        print(person + \"/\" + person_img + \" can't be used for training\")\n",
        "\n",
        "clf = svm.SVC(gamma ='scale')\n",
        "clf.fit(encodings, names)\n",
        "\n",
        "# Load the test image with unknown faces into a numpy array\n",
        "test_image = cv2.imread(\"./Detect/raffidanjokowi.jpg\")\n",
        "test_image = cv2.resize(test_image,(1000,1200),None,0.25,0.25)\n",
        "test_image = cv2.cvtColor(test_image,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Find all the faces in the test image using the default HOG-based model\n",
        "face_locations = face_recognition.face_locations(test_image)\n",
        "no = len(face_locations)\n",
        "print(\"Number of faces detected: \", no)\n",
        "\n",
        "# Predict all the faces in the test image using the trained classifier\n",
        "\n",
        "# facesCurFrame = face_recognition.face_locations(test_image,model=\"cnn\")\n",
        "# encodes  = face_recognition.face_encodings(test_image,face_locations)\n",
        "(locs, preds) = detect_and_predict_mask(test_image, faceNet, maskNet,THRESHOLD)\n",
        "print(\"Found:\")\n",
        "for pred,face_loc in zip(preds,face_locations):\n",
        "    (mask,withoutMask) = pred\n",
        "    for i in range(no):\n",
        "      encodings = face_recognition.face_encodings(test_image)[i]\n",
        "      name = clf.predict([encodings])\n",
        "      label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "      color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "      label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "      color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "      y1,x2,y2,x1 = face_loc\n",
        "      # y1,x2,y2,x1 = y1*4 , x2*4 , y2*4 , x1*4\n",
        "      cv2.rectangle(test_image,(x1,y1),(x2,y2),color,2)\n",
        "      cv2.rectangle(test_image,(x1,y2-35),(x2,y2),color, cv2.FILLED)\n",
        "      cv2.putText(test_image, *name, (x1+6 , y2 - 6),cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255),2 )\n",
        "      cv2.putText(test_image, label, (x1 , y2+10) , cv2.FONT_HERSHEY_COMPLEX , 1, (255,0,255) , 2)\n",
        "      cv2_imshow(test_image)\n",
        "      print(*name)\n",
        "      print(label)\n",
        "    "
      ],
      "metadata": {
        "id": "Z8q32Zad5V6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old Image Detection (Alpha)\n",
        "belum selesai janga di running"
      ],
      "metadata": {
        "id": "kUEm9kJOQqh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img = cv2.imread('./Detect/shakira.jpg')\n",
        "img = cv2.imread(\"./Detect/shakira.jpg\")\n",
        "imgs = cv2.resize(img,(0,0),None,0.25,0.25)\n",
        "imgs = cv2.cvtColor(imgs , cv2.COLOR_BGR2RGB)\n",
        "(locs, preds) = detect_and_predict_mask(imgs, faceNet, maskNet,THRESHOLD)\n",
        "facesCurFrame = face_recognition.face_locations(imgs,model=\"cnn\")\n",
        "encodes  = face_recognition.face_encodings(imgs,facesCurFrame)\n",
        "encodeCurFrame = []\n",
        "if(len(encodes)>0):\n",
        "  encodeCurFrame.append(encodes[0])\n",
        "else:\n",
        "  print(\"no faces found in the image\")\n",
        "\n",
        "for encodeFace , faceLoc , pred in zip(encodeCurFrame,facesCurFrame ,preds):\n",
        "  (mask, withoutMask) = pred\n",
        "  label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "  color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "  label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "  matches = face_recognition.api.compare_faces(encode_list, encodeFace)\n",
        "  faceDis = face_recognition.api.face_distance(encode_list, encodeCurFrame)\n",
        "  print(\"faceDis : \",faceDis)\n",
        "  # matchIndex = np.argmin(faceDis)\n",
        "  # print('index : ',matchIndex)\n",
        "  name = class_names[0]\n",
        "  print('nama : ',name)\n",
        "  color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "  y1,x2,y2,x1 = faceLoc\n",
        "  y1,x2,y2,x1 = y1*4 , x2*4 , y2*4 , x1*4\n",
        "  cv2.rectangle(img,(x1,y1),(x2,y2),color,2)\n",
        "  cv2.rectangle(img,(x1,y2-35),(x2,y2),color, cv2.FILLED)\n",
        "  cv2.putText(img, name, (x1+6 , y2 - 6),cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255),2 )\n",
        "  cv2.putText(img, label, (x1 , y2+10) , cv2.FONT_HERSHEY_COMPLEX , 1 , (255,0,255) , 2)\n",
        "  \n",
        "  plt.imshow(img)\n",
        "  if(matches==True):\n",
        "      print(\"yes is me\")\n",
        "      print()\n",
        "  else:\n",
        "      print(\"no me\")\n",
        "\t\t\n",
        "    "
      ],
      "metadata": {
        "id": "3n59i26sKd92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Detection With Helper (Pre-Alpha)"
      ],
      "metadata": {
        "id": "T-4vQ4ocQwg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encodings image\n",
        "bisa di running"
      ],
      "metadata": {
        "id": "4cz0pKZNXs4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodings=[]\n",
        "names = []\n",
        "for person in myList:\n",
        "  pix = os.listdir(img_path+\"/\"+person)\n",
        "  for person_img in pix:\n",
        "    # Get the face encodings for the face in each image file\n",
        "    path = img_path+\"/\"+person+\"/\"+person_img\n",
        "    face = face_recognition.load_image_file(path)\n",
        "    face_bounding_boxes = face_recognition.face_locations(face)\n",
        "\n",
        "    # If training image contains exactly one face\n",
        "    if len(face_bounding_boxes) == 1:\n",
        "        face_enc = face_recognition.face_encodings(face)[0]\n",
        "        # Add face encoding for current image \n",
        "        # with corresponding label (name) to the training data\n",
        "        encodings.append(face_enc)\n",
        "        names.append(person)\n",
        "    else:\n",
        "        print(person + \"/\" + person_img + \" can't be used for training\")\n",
        "\n",
        "clf = svm.SVC(gamma ='scale')\n",
        "clf.fit(encodings, names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfaGovV3Xr0F",
        "outputId": "dc5ede32-2318-42b0-ff51-d7c9fefc522f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jokowi/jokowipakemasker.jpeg can't be used for training\n",
            "Jokowi/jokowipakemasker.png can't be used for training\n",
            "Raffi/raffi6.jpg can't be used for training\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stream\n",
        "bisa di running"
      ],
      "metadata": {
        "id": "9jtL8Gf_Xwyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # call our darknet helper on video frame\n",
        "    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet,THRESHOLD)\n",
        "    # Find all the faces in the test image using the default HOG-based model\n",
        "    face_locations = face_recognition.face_locations(frame)\n",
        "    no = len(face_locations)\n",
        "    print(\"Number of faces detected: \", no)\n",
        "    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet,THRESHOLD)\n",
        "    print(\"Found:\")\n",
        "\n",
        "    # loop through detections and draw them on transparent overlay image\n",
        "    for pred,face_loc in zip(preds,face_locations):\n",
        "      (mask,withoutMask) = pred\n",
        "      for i in range(no):\n",
        "        encodings = face_recognition.face_encodings(frame)[i]\n",
        "        name = clf.predict([encodings])\n",
        "        label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
        "\n",
        "      y1,x2,y2,x1 = face_loc\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x1,y1),(x2,y2),color,2)\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x1,y2-35),(x2,y2),color, cv2.FILLED)\n",
        "      bbox_array = cv2.putText(bbox_array, *name, (x1+6 , y2 - 6),cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255),2 )\n",
        "      bbox_array = cv2.putText(bbox_array, label, (x1 , y2+10) , cv2.FONT_HERSHEY_COMPLEX , 1, (255,0,255) , 2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "metadata": {
        "id": "cWqROCITQNQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Detection with helper (Pre-Alpha)\n",
        "belom selesai jangan di running"
      ],
      "metadata": {
        "id": "PF5jETZLQmem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  filename = take_photo('photo.jpg')\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "7pDjM2YiQlP9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}